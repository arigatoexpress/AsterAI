# Triton Inference Server Model Configuration for HFT CNN Predictor
# Optimized for sub-2ms inference latency

name: "hft_cnn_predictor"
platform: "tensorrt_plan"
max_batch_size: 32
default_model_filename: "model.plan"

# Input Configuration
input [
  {
    name: "input"
    data_type: TYPE_FP32
    dims: [ 60, 9 ]  # sequence_length=60, features=9
    
    # Optimization: Allow batching for throughput
    allow_ragged_batch: false
  }
]

# Output Configuration
output [
  {
    name: "output"
    data_type: TYPE_FP32
    dims: [ 3 ]  # 3 classes: down, neutral, up
    
    # Label mapping
    label_filename: "labels.txt"
  }
]

# Instance Group Configuration
instance_group [
  {
    name: "hft_cnn_gpu"
    count: 1  # Single instance for L4 GPU
    kind: KIND_GPU
    gpus: [ 0 ]  # Use first GPU
    
    # Performance optimization
    profile: "performance"
  }
]

# Dynamic Batching for Throughput
# Note: May increase latency slightly, disable for ultra-low latency
dynamic_batching {
  preferred_batch_size: [ 1, 4, 8 ]
  max_queue_delay_microseconds: 100  # 100Î¼s max queue delay
  preserve_ordering: true
  priority_levels: 2
  default_priority_level: 1
  
  # For HFT: Consider disabling dynamic batching
  # and using batch_size=1 for lowest latency
}

# Model Optimization Configuration
optimization {
  # Enable CUDA graph optimization (2x speedup)
  graph {
    level: 1
  }
  
  # Input/output memory optimization
  input_pinned_memory {
    enable: true
  }
  output_pinned_memory {
    enable: true
  }
  
  # GPU memory pool
  cuda {
    graphs: true
    busy_wait_events: true
    graph_spec {
      batch_size: 1
      input {
        key: "input"
        value: {
          dim: [ 60, 9 ]
        }
      }
    }
  }
  
  # TensorRT specific optimizations
  execution_accelerators {
    gpu_execution_accelerator {
      name: "tensorrt"
      parameters {
        key: "precision_mode"
        value: "FP16"  # Use FP16 for 2x speedup on L4
      }
      parameters {
        key: "max_workspace_size_bytes"
        value: "8589934592"  # 8GB workspace
      }
      parameters {
        key: "trt_engine_cache_enable"
        value: "1"
      }
    }
  }
}

# Model Warmup Configuration
model_warmup [
  {
    name: "warmup_batch_1"
    batch_size: 1
    inputs {
      key: "input"
      value: {
        data_type: TYPE_FP32
        dims: [ 60, 9 ]
        zero_data: true
      }
    }
    count: 100  # Warmup with 100 iterations
  },
  {
    name: "warmup_batch_8"
    batch_size: 8
    inputs {
      key: "input"
      value: {
        data_type: TYPE_FP32
        dims: [ 60, 9 ]
        zero_data: true
      }
    }
    count: 50
  }
]

# Rate Limiting (Optional)
# Useful to prevent overload
rate_limiter {
  resources [
    {
      name: "gpu_memory"
      global: false
      count: 1
    }
  ]
}

# Model Repository Configuration
model_repository_agents {
  agents [
    {
      name: "gcs"  # Google Cloud Storage
      parameters {
        key: "gcs_bucket"
        value: "gs://hft-models-PROJECT_ID/triton_models"
      }
    }
  ]
}

# Response Cache (Optional)
# For repeated predictions, can reduce latency
# WARNING: Not recommended for HFT where data changes rapidly
# response_cache {
#   enable: false
# }

# Model Control Configuration
model_control {
  mode: EXPLICIT  # Explicit loading for control
  config {
    name: "hft_cnn_predictor"
  }
}

# Versioning
version_policy {
  latest {
    num_versions: 2  # Keep 2 latest versions for rollback
  }
}

# Tags for Model Management
tags {
  key: "application"
  value: "hft_trading"
}
tags {
  key: "model_type"
  value: "price_predictor"
}
tags {
  key: "latency_target_ms"
  value: "2"
}
tags {
  key: "accuracy_target"
  value: "0.85"
}

# Performance Targets
# These are used for monitoring and alerting
performance_targets {
  name: "latency_p50"
  value: 1.0  # 1ms at p50
}
performance_targets {
  name: "latency_p95"
  value: 2.0  # 2ms at p95
}
performance_targets {
  name: "latency_p99"
  value: 3.0  # 3ms at p99
}
performance_targets {
  name: "throughput"
  value: 1000  # 1000 requests/sec minimum
}

# Logging Configuration
model_operations {
  op_library_filename: "libhft_ops.so"  # Custom ops if needed
}

# Metrics Configuration
model_transaction_policy {
  decoupled: false  # For synchronous requests
}

# Backend Configuration for TensorRT
backend: "tensorrt"

parameters {
  key: "TRT_MAX_WORKSPACE_SIZE_BYTES"
  value: { string_value: "8589934592" }  # 8GB
}
parameters {
  key: "TRT_ENABLE_FP16"
  value: { string_value: "1" }
}
parameters {
  key: "TRT_ENGINE_CACHE_DIR"
  value: { string_value: "/models/hft_cnn_predictor/engine_cache" }
}
parameters {
  key: "TRT_ENGINE_CACHE_ENABLE"
  value: { string_value: "1" }
}

# Sequence Batching (if needed for stateful models)
# sequence_batching {
#   max_sequence_idle_microseconds: 5000000  # 5 seconds
#   oldest {
#     max_candidate_sequences: 256
#     max_queue_delay_microseconds: 1000  # 1ms
#   }
# }

